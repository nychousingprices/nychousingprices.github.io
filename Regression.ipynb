{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true;\n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "}\n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ONLY USE THIS BLOCK OF CODE ON NICASIA'S COMPUTER \n",
    "#import sys\n",
    "#sys.path.append(\"/anaconda/lib/python2.7/site-packages\")\n",
    "#####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.patches import Polygon\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler as Standardize\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Loading Data \n",
    "test_data = pd.read_csv(\"data/test_imputed.csv\")\n",
    "train_data = pd.read_csv(\"data/train_imputed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Separating our predictor and response variables \n",
    "x_train = train_data.drop(['zestimate_amount'],1)\n",
    "y_train = train_data['zestimate_amount']\n",
    "x_test = test_data.drop(['zestimate_amount'],1)\n",
    "y_test = test_data['zestimate_amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our baseline model, we implemented a simple linear regression model. Unsurprisingly, our results were not too great. We did not deal with issues such as overfitting, a very likely concern since we have over 100 features.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain Regression: R^2 score on training set 0.198558553055\n",
      "Plain Regression: R^2 score on test set 0.207981303857\n"
     ]
    }
   ],
   "source": [
    "# Fit plain regression on train set, evaluate on train and test sets\n",
    "reg = Lin_Reg()\n",
    "reg.fit(x_train.values, y_train.values)\n",
    "\n",
    "train_r_squared_plain = reg.score(x_train, y_train)\n",
    "test_r_squared_plain = reg.score(x_test, y_test)\n",
    "\n",
    "print 'Plain Regression: R^2 score on training set', train_r_squared_plain\n",
    "print 'Plain Regression: R^2 score on test set', test_r_squared_plain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with overfitting, we implemented Ridge and Lasso Regression with different parameters. Additionally, we will reduce the set of predictors we will incorporate in our model. Starting with 143 predictors, we ended up with 46 final features by dropping predictors with a correlation of less than 0.1 with the response variable. We believe that eliminating weakly related predictors would reduce the noise in our model and improve our performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loading Data \n",
    "test_data = pd.read_csv(\"data/test_best_features.csv\")\n",
    "train_data = pd.read_csv(\"data/train_best_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement Ridge Regression with cross validation and optimize our score with different regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#--------  k_fold_r_squared\n",
    "# A function for k-fold cross validation with Ridge regression\n",
    "# Input: \n",
    "#      x_train (n x d array of predictors in training data)\n",
    "#      y_train (n x 1 array of response variable vals in training data)\n",
    "#      num_folds (no. of folds for CV)\n",
    "#      param_val (regularization parameter value)\n",
    "# Return: \n",
    "#      average R^2 value across folds\n",
    "\n",
    "def k_fold_r_squared(x_train, y_train, num_folds, param_val):\n",
    "    n_train = x_train.shape[0]\n",
    "    n = int(np.round(n_train * 1. / num_folds)) # points per fold\n",
    "\n",
    "    # Iterate over folds\n",
    "    cv_r_squared = 0\n",
    "    \n",
    "    for fold in range(1, num_folds + 1):\n",
    "        # Take k-1 folds for training \n",
    "        x_first_half = x_train[:n * (fold - 1), :]\n",
    "        x_second_half = x_train[n * fold + 1:, :]\n",
    "        x_train_cv = np.concatenate((x_first_half, x_second_half), axis=0)\n",
    "        \n",
    "        y_first_half = y_train[:n * (fold - 1)]\n",
    "        y_second_half = y_train[n * fold + 1:]\n",
    "        y_train_cv = np.concatenate((y_first_half, y_second_half), axis=0)\n",
    "        \n",
    "        # Take the middle fold for testing\n",
    "        x_test_cv = x_train[1 + n * (fold - 1):n * fold, :]\n",
    "        y_test_cv = y_train[1 + n * (fold - 1):n * fold]\n",
    "\n",
    "        # Fit ridge regression model with parameter value on CV train set, and evaluate CV test performance\n",
    "        reg = Ridge_Reg(alpha = param_val)\n",
    "        reg.fit(x_train_cv, y_train_cv)\n",
    "        r_squared = reg.score(x_test_cv, y_test_cv)\n",
    "    \n",
    "        # Cummulative R^2 value across folds\n",
    "        cv_r_squared += r_squared\n",
    "\n",
    "    # Return average R^2 value across folds\n",
    "    return cv_r_squared * 1.0 / num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store test & CV R^2 values for different regression parameter values\n",
    "# Range: 10^-7, ... 10^7\n",
    "max_pow_of_10 = 7\n",
    "min_pow_of_10 = -7\n",
    "num_params = max_pow_of_10 - min_pow_of_10 + 1\n",
    "\n",
    "cv_r_squared = []\n",
    "test_r_squared = []\n",
    "\n",
    "# Iterate over various parameter values\n",
    "for power_of_10 in range(min_pow_of_10, max_pow_of_10+1):\n",
    "    \n",
    "    #standardize x_train and y_train\n",
    "#    std = Standardize(with_mean=False)\n",
    "#    x_train_std = std.fit_transform(x_train)\n",
    "#    x_test_std = x_test / std.scale_ \n",
    "    \n",
    "    # Fit regression model on train set, and evaluate test R^2\n",
    "    reg = Ridge_Reg(alpha=10**power_of_10)\n",
    "    reg.fit(x_train, y_train)\n",
    "    test_r_squared.append(reg.score(x_test, y_test))\n",
    "    \n",
    "    # Evaluate 5-fold CV R^2\n",
    "    cv_r_squared.append(k_fold_r_squared(x_train.values, y_train, 5, 10**power_of_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: Test R^2 score for CV choice 0.231877645585\n",
      "Ridge regression: Max Test R^2 score 0.262278519717\n",
      "Plain regression: Test R^2 score: 0.207981303857\n"
     ]
    }
   ],
   "source": [
    "# Plot CV and test R^2 values as a function of parameter value\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "#ax.semilogx(10.0**np.arange(min_pow_of_10, max_pow_of_10 + 1), \n",
    "           # cv_r_squared, \n",
    "           # c='b', \n",
    "           # label='5-fold CV')\n",
    "#ax.semilogx(10.0**np.arange(min_pow_of_10, max_pow_of_10 + 1), \n",
    "           # test_r_squared, \n",
    "           # c='r', \n",
    "           # label='Test')\n",
    "\n",
    "#ax.set_xlabel('Regularization parameter')\n",
    "#ax.set_ylabel('R^2 score')\n",
    "#ax.set_title('Comparison of R^2 Score')\n",
    "#ax.legend(loc='lower right')\n",
    "# Best CV parameter value\n",
    "best_cv_param = np.argmax(cv_r_squared)\n",
    "\n",
    "# Print R^2 for best CV parameter, max R^2 across all parameters, and R^2 for plain regression\n",
    "print 'Ridge regression: Test R^2 score for CV choice', test_r_squared[best_cv_param]\n",
    "print 'Ridge regression: Max Test R^2 score', max(test_r_squared)\n",
    "print 'Plain regression: Test R^2 score:', test_r_squared_plain\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement Lasso Regression with cross validation and optimize our score with different regularization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  k_fold_r_squared\n",
    "# A function for k-fold cross validation with Ridge regression\n",
    "# Input: \n",
    "#      x_train (n x d array of predictors in training data)\n",
    "#      y_train (n x 1 array of response variable vals in training data)\n",
    "#      num_folds (no. of folds for CV)\n",
    "#      param_val (regularization parameter value)\n",
    "# Return: \n",
    "#      average R^2 value across folds\n",
    "\n",
    "def k_fold_r_squared(x_train, y_train, num_folds, param_val):\n",
    "    n_train = x_train.shape[0]\n",
    "    n = int(np.round(n_train * 1. / num_folds)) # points per fold\n",
    "\n",
    "    # Iterate over folds\n",
    "    cv_r_squared = 0\n",
    "    \n",
    "    for fold in range(1, num_folds + 1):\n",
    "        # Take k-1 folds for training \n",
    "        x_first_half = x_train[:n * (fold - 1), :]\n",
    "        x_second_half = x_train[n * fold + 1:, :]\n",
    "        x_train_cv = np.concatenate((x_first_half, x_second_half), axis=0)\n",
    "        \n",
    "        y_first_half = y_train[:n * (fold - 1)]\n",
    "        y_second_half = y_train[n * fold + 1:]\n",
    "        y_train_cv = np.concatenate((y_first_half, y_second_half), axis=0)\n",
    "        \n",
    "        # Take the middle fold for testing\n",
    "        x_test_cv = x_train[1 + n * (fold - 1):n * fold, :]\n",
    "        y_test_cv = y_train[1 + n * (fold - 1):n * fold]\n",
    "\n",
    "        # Fit ridge regression model with parameter value on CV train set, and evaluate CV test performance\n",
    "        reg = Lasso_Reg(alpha = param_val)\n",
    "        reg.fit(x_train_cv, y_train_cv)\n",
    "        r_squared = reg.score(x_test_cv, y_test_cv)\n",
    "    \n",
    "        # Cummulative R^2 value across folds\n",
    "        cv_r_squared += r_squared\n",
    "\n",
    "    # Return average R^2 value across folds\n",
    "    return cv_r_squared * 1.0 / num_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store test & CV R^2 values for different regression parameter values\n",
    "# Range: 10^-7, ... 10^7\n",
    "max_pow_of_10 = 7\n",
    "min_pow_of_10 = -7\n",
    "num_params = max_pow_of_10 - min_pow_of_10 + 1\n",
    "\n",
    "cv_r_squared = []\n",
    "test_r_squared = []\n",
    "\n",
    "# Iterate over various parameter values\n",
    "for power_of_10 in range(min_pow_of_10, max_pow_of_10+1):\n",
    "    \n",
    "    #standardize x_train and y_train\n",
    "    #std = Standardize(with_mean=False)\n",
    "    #x_train_std = std.fit_transform(x_train)\n",
    "    #x_test_std = x_test / std.scale_ \n",
    "    \n",
    "    # Fit regression model on train set, and evaluate test R^2\n",
    "    reg = Lasso_Reg(alpha=10**power_of_10)\n",
    "    reg.fit(x_train, y_train)\n",
    "    test_r_squared.append(reg.score(x_test, y_test))\n",
    "    \n",
    "    # Evaluate 5-fold CV R^2\n",
    "    cv_r_squared.append(k_fold_r_squared(x_train.values, y_train, 5, 10**power_of_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: Test R^2 score for CV choice 0.231877645585\n",
      "Lasso regression: Max Test R^2 score 0.262278519717\n",
      "Plain regression: Test R^2 score: 0.207981303857\n"
     ]
    }
   ],
   "source": [
    "# Plot CV and test R^2 values as a function of parameter value\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "#ax.semilogx(10.0**np.arange(min_pow_of_10, max_pow_of_10 + 1), \n",
    "#            cv_r_squared, \n",
    "#            c='b', \n",
    "#            label='5-fold CV')\n",
    "#ax.semilogx(10.0**np.arange(min_pow_of_10, max_pow_of_10 + 1), \n",
    "#            test_r_squared, \n",
    "#            c='r', \n",
    "#            label='Test')\n",
    "\n",
    "#ax.set_xlabel('Regularization parameter')\n",
    "#ax.set_ylabel('R^2 score')\n",
    "#ax.set_title('Comparison of R^2 Score')\n",
    "#ax.legend(loc='lower right')\n",
    "\n",
    "# Best CV parameter value\n",
    "best_cv_param = np.argmax(cv_r_squared)\n",
    "\n",
    "# Print R^2 for best CV parameter, max R^2 across all parameters, and R^2 for plain regression\n",
    "print 'Lasso regression: Test R^2 score for CV choice', test_r_squared[best_cv_param]\n",
    "print 'Lasso regression: Max Test R^2 score', max(test_r_squared)\n",
    "print 'Plain regression: Test R^2 score:', test_r_squared_plain\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our Lasso and Ridge regression models make marginal gains on our regression scores, the performance is still fairly low. This may be due to a lack of linear relationship between predictor and response variables. If that is the case, then other non-linear models may be more suitable for our data. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
